# ğŸ¬ ì˜ìƒ ìƒì„± íŒŒì´í”„ë¼ì¸ ìƒì„¸ ì„¤ê³„

## 1. ê°œìš”

### 1.1 ëª©í‘œ
- ìŠ¤í¬ë¦½íŠ¸ â†’ ì™„ì„±ëœ Shorts ì˜ìƒ ìë™ ìƒì„±
- TTS + ìë§‰ + ë°°ê²½ ì˜ìƒ/ì´ë¯¸ì§€ í•©ì„±
- ì±„ë„ë³„ ì¼ê´€ëœ ë¹„ì£¼ì–¼ ìŠ¤íƒ€ì¼ ìœ ì§€
- ì¸ë„¤ì¼ ìë™ ìƒì„±

### 1.2 íŒŒì´í”„ë¼ì¸ íë¦„
```
ìŠ¤í¬ë¦½íŠ¸ â†’ TTS ìƒì„± â†’ ìë§‰ ìƒì„± â†’ ë¹„ì£¼ì–¼ ì†Œì‹± â†’ í•©ì„± â†’ ì¸ë„¤ì¼ â†’ ì¶œë ¥
```

---

## 2. TTS (Text-to-Speech)

### 2.1 TTS ì„œë¹„ìŠ¤ ì¶”ìƒí™”
```python
from abc import ABC, abstractmethod
from pydantic import BaseModel
from enum import Enum
from pathlib import Path


class TTSService(str, Enum):
    EDGE_TTS = "edge-tts"
    ELEVENLABS = "elevenlabs"
    CLOVA = "clova"


class VoiceConfig(BaseModel):
    service: TTSService
    voice_id: str
    speed: float = 1.0          # 0.5 - 2.0
    pitch: float = 0.0          # -20 - 20 (ì¼ë¶€ ì„œë¹„ìŠ¤)

    # ElevenLabs ì „ìš©
    stability: float | None = None
    similarity_boost: float | None = None


class TTSResult(BaseModel):
    audio_path: Path
    duration_seconds: float
    word_timestamps: list["WordTimestamp"] | None = None


class WordTimestamp(BaseModel):
    word: str
    start: float      # ì´ˆ
    end: float        # ì´ˆ


class BaseTTSEngine(ABC):
    @abstractmethod
    async def synthesize(
        self,
        text: str,
        config: VoiceConfig,
        output_path: Path
    ) -> TTSResult:
        pass

    @abstractmethod
    def get_available_voices(self) -> list[dict]:
        pass
```

### 2.2 Edge TTS êµ¬í˜„ (ë¬´ë£Œ)
```python
import edge_tts
import asyncio
from pathlib import Path


class EdgeTTSEngine(BaseTTSEngine):
    """ë¬´ë£Œ Microsoft Edge TTS"""

    # ì¶”ì²œ í•œêµ­ì–´ ìŒì„±
    KOREAN_VOICES = {
        "male": [
            "ko-KR-InJoonNeural",      # ë‚¨ì„±, ìì—°ìŠ¤ëŸ¬ì›€
            "ko-KR-BongJinNeural",     # ë‚¨ì„±, ì°¨ë¶„í•¨
            "ko-KR-GookMinNeural",     # ë‚¨ì„±, ë°ìŒ
        ],
        "female": [
            "ko-KR-SunHiNeural",       # ì—¬ì„±, ìì—°ìŠ¤ëŸ¬ì›€
            "ko-KR-JiMinNeural",       # ì—¬ì„±, ë°ìŒ
            "ko-KR-SeoHyeonNeural",    # ì—¬ì„±, ì°¨ë¶„í•¨
            "ko-KR-YuJinNeural",       # ì—¬ì„±, ë˜ë ·í•¨
        ],
    }

    async def synthesize(
        self,
        text: str,
        config: VoiceConfig,
        output_path: Path
    ) -> TTSResult:
        # ì†ë„ ë³€í™˜ (1.0 â†’ "+0%", 1.2 â†’ "+20%")
        rate = f"{int((config.speed - 1) * 100):+d}%"

        communicate = edge_tts.Communicate(
            text=text,
            voice=config.voice_id,
            rate=rate,
        )

        # ì˜¤ë””ì˜¤ + ìë§‰ ë°ì´í„° ìƒì„±
        audio_path = output_path.with_suffix(".mp3")

        word_timestamps = []

        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                with open(audio_path, "ab") as f:
                    f.write(chunk["data"])
            elif chunk["type"] == "WordBoundary":
                word_timestamps.append(WordTimestamp(
                    word=chunk["text"],
                    start=chunk["offset"] / 10_000_000,  # 100ns â†’ ì´ˆ
                    end=(chunk["offset"] + chunk["duration"]) / 10_000_000,
                ))

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
        duration = await self._get_audio_duration(audio_path)

        return TTSResult(
            audio_path=audio_path,
            duration_seconds=duration,
            word_timestamps=word_timestamps,
        )

    async def _get_audio_duration(self, path: Path) -> float:
        """ffprobeë¡œ ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸"""
        import subprocess
        result = subprocess.run(
            ["ffprobe", "-v", "quiet", "-show_entries",
             "format=duration", "-of", "csv=p=0", str(path)],
            capture_output=True, text=True
        )
        return float(result.stdout.strip())

    def get_available_voices(self) -> list[dict]:
        return [
            {"id": v, "gender": g, "language": "ko-KR"}
            for g, voices in self.KOREAN_VOICES.items()
            for v in voices
        ]
```

### 2.3 ElevenLabs êµ¬í˜„ (ê³ í’ˆì§ˆ)
```python
from elevenlabs import generate, set_api_key, Voice, VoiceSettings


class ElevenLabsEngine(BaseTTSEngine):
    """ê³ í’ˆì§ˆ AI ìŒì„± (ìœ ë£Œ)"""

    def __init__(self, api_key: str):
        set_api_key(api_key)

    async def synthesize(
        self,
        text: str,
        config: VoiceConfig,
        output_path: Path
    ) -> TTSResult:
        voice_settings = VoiceSettings(
            stability=config.stability or 0.5,
            similarity_boost=config.similarity_boost or 0.75,
        )

        audio = generate(
            text=text,
            voice=Voice(
                voice_id=config.voice_id,
                settings=voice_settings,
            ),
            model="eleven_multilingual_v2",  # ë‹¤êµ­ì–´ ì§€ì›
        )

        audio_path = output_path.with_suffix(".mp3")
        with open(audio_path, "wb") as f:
            f.write(audio)

        duration = await self._get_audio_duration(audio_path)

        # ElevenLabsëŠ” word timestamp ë¯¸ì§€ì› â†’ Whisperë¡œ ìƒì„±
        word_timestamps = await self._generate_timestamps_with_whisper(audio_path)

        return TTSResult(
            audio_path=audio_path,
            duration_seconds=duration,
            word_timestamps=word_timestamps,
        )

    async def _generate_timestamps_with_whisper(
        self,
        audio_path: Path
    ) -> list[WordTimestamp]:
        """Whisperë¡œ ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±"""
        import whisper

        model = whisper.load_model("base")  # ë˜ëŠ” "small"
        result = model.transcribe(
            str(audio_path),
            language="ko",
            word_timestamps=True,
        )

        timestamps = []
        for segment in result["segments"]:
            for word_info in segment.get("words", []):
                timestamps.append(WordTimestamp(
                    word=word_info["word"],
                    start=word_info["start"],
                    end=word_info["end"],
                ))

        return timestamps
```

### 2.4 TTS íŒ©í† ë¦¬
```python
class TTSEngineFactory:
    _engines: dict[TTSService, BaseTTSEngine] = {}

    @classmethod
    def get_engine(cls, service: TTSService) -> BaseTTSEngine:
        if service not in cls._engines:
            if service == TTSService.EDGE_TTS:
                cls._engines[service] = EdgeTTSEngine()
            elif service == TTSService.ELEVENLABS:
                from config import settings
                cls._engines[service] = ElevenLabsEngine(settings.elevenlabs_api_key)
            else:
                raise ValueError(f"Unknown TTS service: {service}")

        return cls._engines[service]
```

---

## 3. ìë§‰ ìƒì„±

### 3.1 ìë§‰ ìŠ¤í‚¤ë§ˆ
```python
from pydantic import BaseModel
from enum import Enum


class SubtitleStyle(BaseModel):
    """ìë§‰ ìŠ¤íƒ€ì¼ ì„¤ì •"""
    font_name: str = "Pretendard"
    font_size: int = 48
    font_color: str = "#FFFFFF"

    # ì™¸ê³½ì„ 
    outline_color: str = "#000000"
    outline_width: int = 2

    # ê·¸ë¦¼ì
    shadow_color: str = "#000000"
    shadow_offset: int = 2

    # ë°°ê²½ ë°•ìŠ¤
    background_enabled: bool = True
    background_color: str = "#000000"
    background_opacity: float = 0.7
    background_padding: int = 10

    # ìœ„ì¹˜
    position: str = "bottom"  # bottom, center, top
    margin_bottom: int = 50
    margin_horizontal: int = 30

    # ì• ë‹ˆë©”ì´ì…˜
    highlight_current_word: bool = True
    highlight_color: str = "#FFFF00"
    fade_in: bool = False

    # ì¤„ë°”ê¿ˆ
    max_chars_per_line: int = 20
    max_lines: int = 2


class SubtitleSegment(BaseModel):
    """ìë§‰ ì„¸ê·¸ë¨¼íŠ¸"""
    index: int
    start: float        # ì´ˆ
    end: float          # ì´ˆ
    text: str
    words: list[WordTimestamp] | None = None


class SubtitleFile(BaseModel):
    """ìë§‰ íŒŒì¼"""
    segments: list[SubtitleSegment]
    style: SubtitleStyle
    format: str = "ass"  # ass, srt
```

### 3.2 ìë§‰ ìƒì„±ê¸°
```python
import re
from pathlib import Path


class SubtitleGenerator:
    def __init__(self, style: SubtitleStyle | None = None):
        self.style = style or SubtitleStyle()

    def generate_from_timestamps(
        self,
        word_timestamps: list[WordTimestamp],
        style: SubtitleStyle | None = None
    ) -> SubtitleFile:
        """ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ â†’ ìë§‰ ì„¸ê·¸ë¨¼íŠ¸"""
        style = style or self.style
        segments = []

        current_segment_words = []
        current_text = ""
        segment_start = None

        for word in word_timestamps:
            test_text = current_text + word.word

            # ì¤„ë°”ê¿ˆ í•„ìš” ì²´í¬
            if len(test_text) > style.max_chars_per_line * style.max_lines:
                # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
                if current_segment_words:
                    segments.append(SubtitleSegment(
                        index=len(segments) + 1,
                        start=segment_start,
                        end=current_segment_words[-1].end,
                        text=current_text.strip(),
                        words=current_segment_words.copy(),
                    ))

                # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
                current_segment_words = [word]
                current_text = word.word
                segment_start = word.start
            else:
                if segment_start is None:
                    segment_start = word.start
                current_segment_words.append(word)
                current_text = test_text

        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸
        if current_segment_words:
            segments.append(SubtitleSegment(
                index=len(segments) + 1,
                start=segment_start,
                end=current_segment_words[-1].end,
                text=current_text.strip(),
                words=current_segment_words,
            ))

        return SubtitleFile(segments=segments, style=style)

    def generate_from_script(
        self,
        script: str,
        audio_duration: float,
        style: SubtitleStyle | None = None
    ) -> SubtitleFile:
        """ìŠ¤í¬ë¦½íŠ¸ + ì˜¤ë””ì˜¤ ê¸¸ì´ â†’ ê· ë“± ë¶„í•  ìë§‰ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ì„ ë•Œ)"""
        style = style or self.style

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]\s*', script)
        sentences = [s.strip() for s in sentences if s.strip()]

        # ê· ë“± ì‹œê°„ ë°°ë¶„
        time_per_sentence = audio_duration / len(sentences)

        segments = []
        for i, sentence in enumerate(sentences):
            segments.append(SubtitleSegment(
                index=i + 1,
                start=i * time_per_sentence,
                end=(i + 1) * time_per_sentence,
                text=self._wrap_text(sentence, style.max_chars_per_line),
            ))

        return SubtitleFile(segments=segments, style=style)

    def _wrap_text(self, text: str, max_chars: int) -> str:
        """ê¸´ í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ"""
        words = text.split()
        lines = []
        current_line = ""

        for word in words:
            if len(current_line) + len(word) + 1 <= max_chars:
                current_line += (" " if current_line else "") + word
            else:
                if current_line:
                    lines.append(current_line)
                current_line = word

        if current_line:
            lines.append(current_line)

        return "\n".join(lines)

    def to_ass(self, subtitle: SubtitleFile, output_path: Path) -> Path:
        """ASS í¬ë§·ìœ¼ë¡œ ì €ì¥ (ìŠ¤íƒ€ì¼ë§ ì§€ì›)"""
        style = subtitle.style

        # ASS í—¤ë”
        ass_content = f"""[Script Info]
Title: Generated Subtitle
ScriptType: v4.00+
PlayDepth: 0

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,{style.font_name},{style.font_size},{self._color_to_ass(style.font_color)},{self._color_to_ass(style.highlight_color)},{self._color_to_ass(style.outline_color)},{self._color_to_ass(style.background_color, style.background_opacity)},0,0,0,0,100,100,0,0,1,{style.outline_width},{style.shadow_offset},2,{style.margin_horizontal},{style.margin_horizontal},{style.margin_bottom},1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""

        # ìë§‰ ì´ë²¤íŠ¸
        for seg in subtitle.segments:
            start = self._seconds_to_ass_time(seg.start)
            end = self._seconds_to_ass_time(seg.end)
            text = seg.text.replace("\n", "\\N")

            ass_content += f"Dialogue: 0,{start},{end},Default,,0,0,0,,{text}\n"

        output_path = output_path.with_suffix(".ass")
        output_path.write_text(ass_content, encoding="utf-8")
        return output_path

    def to_srt(self, subtitle: SubtitleFile, output_path: Path) -> Path:
        """SRT í¬ë§·ìœ¼ë¡œ ì €ì¥ (ë‹¨ìˆœ)"""
        srt_content = ""

        for seg in subtitle.segments:
            start = self._seconds_to_srt_time(seg.start)
            end = self._seconds_to_srt_time(seg.end)
            srt_content += f"{seg.index}\n{start} --> {end}\n{seg.text}\n\n"

        output_path = output_path.with_suffix(".srt")
        output_path.write_text(srt_content, encoding="utf-8")
        return output_path

    def _color_to_ass(self, hex_color: str, alpha: float = 1.0) -> str:
        """HEX â†’ ASS ì»¬ëŸ¬ (&HAABBGGRR)"""
        hex_color = hex_color.lstrip("#")
        r, g, b = int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)
        a = int((1 - alpha) * 255)
        return f"&H{a:02X}{b:02X}{g:02X}{r:02X}"

    def _seconds_to_ass_time(self, seconds: float) -> str:
        """ì´ˆ â†’ ASS ì‹œê°„ (H:MM:SS.CC)"""
        h = int(seconds // 3600)
        m = int((seconds % 3600) // 60)
        s = int(seconds % 60)
        cs = int((seconds % 1) * 100)
        return f"{h}:{m:02d}:{s:02d}.{cs:02d}"

    def _seconds_to_srt_time(self, seconds: float) -> str:
        """ì´ˆ â†’ SRT ì‹œê°„ (HH:MM:SS,mmm)"""
        h = int(seconds // 3600)
        m = int((seconds % 3600) // 60)
        s = int(seconds % 60)
        ms = int((seconds % 1) * 1000)
        return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"
```

---

## 4. ë¹„ì£¼ì–¼ ì†Œì‹±

### 4.1 ë¹„ì£¼ì–¼ ì†ŒìŠ¤ íƒ€ì…
```python
from enum import Enum
from pydantic import BaseModel
from pathlib import Path


class VisualSourceType(str, Enum):
    STOCK_VIDEO = "stock_video"      # Pexels, Pixabay
    STOCK_IMAGE = "stock_image"      # ì •ì  ì´ë¯¸ì§€
    AI_IMAGE = "ai_image"            # DALL-E ìƒì„±
    SOLID_COLOR = "solid_color"      # ë‹¨ìƒ‰ ë°°ê²½
    GRADIENT = "gradient"            # ê·¸ë¼ë°ì´ì…˜


class VisualAsset(BaseModel):
    type: VisualSourceType
    path: Path | None = None
    url: str | None = None
    duration: float | None = None    # ë¹„ë””ì˜¤ì¸ ê²½ìš°

    # ì´ë¯¸ì§€/ë‹¨ìƒ‰ ì˜µì…˜
    color: str | None = None
    gradient_colors: list[str] | None = None

    # ë©”íƒ€
    source: str | None = None        # "pexels", "dalle", etc
    license: str | None = None
    keywords: list[str] = []


class VisualConfig(BaseModel):
    """ì±„ë„ë³„ ë¹„ì£¼ì–¼ ì„¤ì •"""

    # ê¸°ë³¸ ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„
    source_priority: list[VisualSourceType] = [
        VisualSourceType.STOCK_VIDEO,
        VisualSourceType.AI_IMAGE,
        VisualSourceType.SOLID_COLOR,
    ]

    # ìŠ¤í†¡ ì„¤ì •
    stock_config: dict = {
        "orientation": "portrait",    # Shortsìš© ì„¸ë¡œ
        "min_duration": 5,
        "max_results": 10,
    }

    # AI ì´ë¯¸ì§€ ì„¤ì •
    ai_image_config: dict = {
        "model": "dall-e-3",
        "size": "1024x1792",          # ì„¸ë¡œ
        "quality": "standard",
    }

    # í´ë°± ì„¤ì •
    fallback_color: str = "#1a1a2e"
    fallback_gradient: list[str] = ["#1a1a2e", "#16213e"]
```

### 4.2 ìŠ¤í†¡ ì˜ìƒ/ì´ë¯¸ì§€ ì†Œì‹±
```python
import httpx
from typing import AsyncIterator


class PexelsClient:
    """Pexels API í´ë¼ì´ì–¸íŠ¸ (ë¬´ë£Œ)"""

    BASE_URL = "https://api.pexels.com"

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = httpx.AsyncClient(
            headers={"Authorization": api_key}
        )

    async def search_videos(
        self,
        query: str,
        orientation: str = "portrait",
        min_duration: int = 5,
        max_results: int = 10,
    ) -> list[VisualAsset]:
        """ë¹„ë””ì˜¤ ê²€ìƒ‰"""
        response = await self.client.get(
            f"{self.BASE_URL}/videos/search",
            params={
                "query": query,
                "orientation": orientation,
                "per_page": max_results,
            }
        )
        response.raise_for_status()
        data = response.json()

        assets = []
        for video in data.get("videos", []):
            # HD ë²„ì „ ì„ íƒ
            video_file = next(
                (f for f in video["video_files"]
                 if f["quality"] == "hd" and f["width"] < f["height"]),
                video["video_files"][0] if video["video_files"] else None
            )

            if video_file and video["duration"] >= min_duration:
                assets.append(VisualAsset(
                    type=VisualSourceType.STOCK_VIDEO,
                    url=video_file["link"],
                    duration=video["duration"],
                    source="pexels",
                    license="Pexels License",
                    keywords=query.split(),
                ))

        return assets

    async def search_images(
        self,
        query: str,
        orientation: str = "portrait",
        max_results: int = 10,
    ) -> list[VisualAsset]:
        """ì´ë¯¸ì§€ ê²€ìƒ‰"""
        response = await self.client.get(
            f"{self.BASE_URL}/v1/search",
            params={
                "query": query,
                "orientation": orientation,
                "per_page": max_results,
            }
        )
        response.raise_for_status()
        data = response.json()

        assets = []
        for photo in data.get("photos", []):
            assets.append(VisualAsset(
                type=VisualSourceType.STOCK_IMAGE,
                url=photo["src"]["large2x"],
                source="pexels",
                license="Pexels License",
                keywords=query.split(),
            ))

        return assets

    async def download(self, url: str, output_path: Path) -> Path:
        """ì—ì…‹ ë‹¤ìš´ë¡œë“œ"""
        response = await self.client.get(url)
        response.raise_for_status()

        output_path.write_bytes(response.content)
        return output_path


class AIImageGenerator:
    """DALL-E ì´ë¯¸ì§€ ìƒì„±"""

    def __init__(self, api_key: str):
        from openai import AsyncOpenAI
        self.client = AsyncOpenAI(api_key=api_key)

    async def generate(
        self,
        prompt: str,
        style: str = "cinematic",
        size: str = "1024x1792",
    ) -> VisualAsset:
        """AI ì´ë¯¸ì§€ ìƒì„±"""
        # í”„ë¡¬í”„íŠ¸ ê°•í™”
        enhanced_prompt = f"{prompt}, {style} style, high quality, vertical format"

        response = await self.client.images.generate(
            model="dall-e-3",
            prompt=enhanced_prompt,
            size=size,
            quality="standard",
            n=1,
        )

        return VisualAsset(
            type=VisualSourceType.AI_IMAGE,
            url=response.data[0].url,
            source="dalle",
            keywords=prompt.split()[:5],
        )
```

### 4.3 ë¹„ì£¼ì–¼ ì†Œì‹± ë§¤ë‹ˆì €
```python
class VisualSourcingManager:
    """ì£¼ì œ/í‚¤ì›Œë“œ ê¸°ë°˜ ë¹„ì£¼ì–¼ ìë™ ì†Œì‹±"""

    def __init__(
        self,
        pexels_client: PexelsClient,
        ai_generator: AIImageGenerator | None = None,
    ):
        self.pexels = pexels_client
        self.ai_generator = ai_generator

    async def source_visuals(
        self,
        keywords: list[str],
        duration_needed: float,
        config: VisualConfig,
    ) -> list[VisualAsset]:
        """í•„ìš”í•œ ê¸¸ì´ë§Œí¼ ë¹„ì£¼ì–¼ ì†Œì‹±"""
        assets = []
        total_duration = 0

        for source_type in config.source_priority:
            if total_duration >= duration_needed:
                break

            if source_type == VisualSourceType.STOCK_VIDEO:
                for keyword in keywords:
                    videos = await self.pexels.search_videos(
                        query=keyword,
                        **config.stock_config,
                    )
                    for video in videos:
                        if total_duration >= duration_needed:
                            break
                        assets.append(video)
                        total_duration += video.duration or 10

            elif source_type == VisualSourceType.STOCK_IMAGE:
                for keyword in keywords:
                    images = await self.pexels.search_images(
                        query=keyword,
                        **config.stock_config,
                    )
                    # ì´ë¯¸ì§€ëŠ” 5ì´ˆì”© ì‚¬ìš©
                    for image in images[:3]:
                        if total_duration >= duration_needed:
                            break
                        image.duration = 5
                        assets.append(image)
                        total_duration += 5

            elif source_type == VisualSourceType.AI_IMAGE and self.ai_generator:
                prompt = " ".join(keywords[:3])
                image = await self.ai_generator.generate(prompt)
                image.duration = duration_needed - total_duration
                assets.append(image)
                total_duration = duration_needed

        # í´ë°±: ë‹¨ìƒ‰ ë°°ê²½
        if total_duration < duration_needed:
            assets.append(VisualAsset(
                type=VisualSourceType.SOLID_COLOR,
                color=config.fallback_color,
                duration=duration_needed - total_duration,
            ))

        return assets
```

---

## 5. ì˜ìƒ í•©ì„± (FFmpeg)

### 5.1 í•©ì„± ì„¤ì •
```python
class VideoConfig(BaseModel):
    """ì˜ìƒ ì¶œë ¥ ì„¤ì •"""

    # í•´ìƒë„ (Shorts = 9:16)
    width: int = 1080
    height: int = 1920

    # ì½”ë±
    video_codec: str = "libx264"
    audio_codec: str = "aac"

    # í’ˆì§ˆ
    crf: int = 23                    # í’ˆì§ˆ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, 18-28)
    preset: str = "medium"           # ì¸ì½”ë”© ì†ë„
    audio_bitrate: str = "192k"

    # í”„ë ˆì„
    fps: int = 30

    # ì¶œë ¥
    format: str = "mp4"


class CompositionConfig(BaseModel):
    """í•©ì„± ì„¤ì •"""
    video: VideoConfig = VideoConfig()
    subtitle_style: SubtitleStyle = SubtitleStyle()

    # íŠ¸ëœì§€ì…˜
    transition_type: str = "fade"    # fade, none
    transition_duration: float = 0.5

    # ë°°ê²½ ì²˜ë¦¬
    blur_background: bool = True     # ê°€ë¡œ ì˜ìƒ â†’ ì„¸ë¡œ ë³€í™˜ ì‹œ ë¸”ëŸ¬ ë°°ê²½

    # ì˜¤ë””ì˜¤
    background_music: Path | None = None
    music_volume: float = 0.1        # ë°°ê²½ ìŒì•… ë³¼ë¥¨ (0-1)
```

### 5.2 FFmpeg í•©ì„±ê¸°
```python
import subprocess
import tempfile
from pathlib import Path


class FFmpegCompositor:
    """FFmpeg ê¸°ë°˜ ì˜ìƒ í•©ì„±"""

    def __init__(self, config: CompositionConfig | None = None):
        self.config = config or CompositionConfig()

    async def compose(
        self,
        audio: TTSResult,
        subtitle: SubtitleFile,
        visuals: list[VisualAsset],
        output_path: Path,
    ) -> Path:
        """ì „ì²´ í•©ì„± íŒŒì´í”„ë¼ì¸"""
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir = Path(tmpdir)

            # 1. ë¹„ì£¼ì–¼ ì—ì…‹ ë‹¤ìš´ë¡œë“œ/ì¤€ë¹„
            visual_paths = await self._prepare_visuals(visuals, tmpdir)

            # 2. ë¹„ì£¼ì–¼ ì‹œí€€ìŠ¤ ìƒì„±
            video_sequence = await self._create_video_sequence(
                visual_paths,
                audio.duration_seconds,
                tmpdir,
            )

            # 3. ìë§‰ íŒŒì¼ ìƒì„±
            subtitle_generator = SubtitleGenerator()
            subtitle_path = subtitle_generator.to_ass(subtitle, tmpdir / "subtitle")

            # 4. ìµœì¢… í•©ì„±
            return await self._final_compose(
                video_path=video_sequence,
                audio_path=audio.audio_path,
                subtitle_path=subtitle_path,
                output_path=output_path,
            )

    async def _prepare_visuals(
        self,
        visuals: list[VisualAsset],
        tmpdir: Path,
    ) -> list[Path]:
        """ë¹„ì£¼ì–¼ ì—ì…‹ ì¤€ë¹„"""
        paths = []

        for i, visual in enumerate(visuals):
            if visual.type == VisualSourceType.SOLID_COLOR:
                # ë‹¨ìƒ‰ ì´ë¯¸ì§€ ìƒì„±
                path = await self._create_solid_color_image(
                    visual.color,
                    tmpdir / f"solid_{i}.png",
                )
            elif visual.url:
                # ë‹¤ìš´ë¡œë“œ
                path = tmpdir / f"asset_{i}{Path(visual.url).suffix or '.mp4'}"
                async with httpx.AsyncClient() as client:
                    response = await client.get(visual.url)
                    path.write_bytes(response.content)
            elif visual.path:
                path = visual.path
            else:
                continue

            paths.append((path, visual.duration or 5))

        return paths

    async def _create_solid_color_image(
        self,
        color: str,
        output_path: Path
    ) -> Path:
        """ë‹¨ìƒ‰ ì´ë¯¸ì§€ ìƒì„±"""
        from PIL import Image

        img = Image.new(
            "RGB",
            (self.config.video.width, self.config.video.height),
            color,
        )
        img.save(output_path)
        return output_path

    async def _create_video_sequence(
        self,
        visual_paths: list[tuple[Path, float]],
        total_duration: float,
        tmpdir: Path,
    ) -> Path:
        """ë¹„ì£¼ì–¼ ì‹œí€€ìŠ¤ â†’ ë‹¨ì¼ ë¹„ë””ì˜¤"""
        cfg = self.config.video

        # ê° ë¹„ì£¼ì–¼ì„ í•„ìš”í•œ ê¸¸ì´ë¡œ ë³€í™˜
        segments = []
        for i, (path, duration) in enumerate(visual_paths):
            segment_path = tmpdir / f"segment_{i}.mp4"

            if path.suffix in [".jpg", ".jpeg", ".png", ".webp"]:
                # ì´ë¯¸ì§€ â†’ ë¹„ë””ì˜¤ ë³€í™˜
                cmd = [
                    "ffmpeg", "-y",
                    "-loop", "1",
                    "-i", str(path),
                    "-c:v", cfg.video_codec,
                    "-t", str(duration),
                    "-vf", f"scale={cfg.width}:{cfg.height}:force_original_aspect_ratio=decrease,pad={cfg.width}:{cfg.height}:(ow-iw)/2:(oh-ih)/2",
                    "-r", str(cfg.fps),
                    "-pix_fmt", "yuv420p",
                    str(segment_path),
                ]
            else:
                # ë¹„ë””ì˜¤ ìŠ¤ì¼€ì¼ë§ + ê¸¸ì´ ì¡°ì •
                cmd = [
                    "ffmpeg", "-y",
                    "-i", str(path),
                    "-c:v", cfg.video_codec,
                    "-t", str(duration),
                    "-vf", f"scale={cfg.width}:{cfg.height}:force_original_aspect_ratio=decrease,pad={cfg.width}:{cfg.height}:(ow-iw)/2:(oh-ih)/2",
                    "-r", str(cfg.fps),
                    "-an",  # ì˜¤ë””ì˜¤ ì œê±°
                    "-pix_fmt", "yuv420p",
                    str(segment_path),
                ]

            subprocess.run(cmd, check=True, capture_output=True)
            segments.append(segment_path)

        # concat íŒŒì¼ ìƒì„±
        concat_file = tmpdir / "concat.txt"
        concat_content = "\n".join(f"file '{p}'" for p in segments)
        concat_file.write_text(concat_content)

        # ë³‘í•©
        output_path = tmpdir / "video_sequence.mp4"
        cmd = [
            "ffmpeg", "-y",
            "-f", "concat",
            "-safe", "0",
            "-i", str(concat_file),
            "-c", "copy",
            str(output_path),
        ]
        subprocess.run(cmd, check=True, capture_output=True)

        return output_path

    async def _final_compose(
        self,
        video_path: Path,
        audio_path: Path,
        subtitle_path: Path,
        output_path: Path,
    ) -> Path:
        """ë¹„ë””ì˜¤ + ì˜¤ë””ì˜¤ + ìë§‰ ìµœì¢… í•©ì„±"""
        cfg = self.config.video

        cmd = [
            "ffmpeg", "-y",
            "-i", str(video_path),
            "-i", str(audio_path),
            "-vf", f"ass={subtitle_path}",
            "-c:v", cfg.video_codec,
            "-crf", str(cfg.crf),
            "-preset", cfg.preset,
            "-c:a", cfg.audio_codec,
            "-b:a", cfg.audio_bitrate,
            "-shortest",
            "-pix_fmt", "yuv420p",
            str(output_path),
        ]

        # ë°°ê²½ ìŒì•… ì¶”ê°€
        if self.config.background_music:
            cmd = self._add_background_music(cmd)

        subprocess.run(cmd, check=True, capture_output=True)
        return output_path

    def _add_background_music(self, cmd: list) -> list:
        """ë°°ê²½ ìŒì•… ë¯¹ì‹±"""
        # TODO: ë°°ê²½ ìŒì•… ë³¼ë¥¨ ì¡°ì ˆ + ë¯¹ì‹±
        return cmd
```

---

## 6. ì¸ë„¤ì¼ ìƒì„±

### 6.1 ì¸ë„¤ì¼ ì„¤ì •
```python
class ThumbnailStyle(BaseModel):
    """ì¸ë„¤ì¼ ìŠ¤íƒ€ì¼"""
    width: int = 1280
    height: int = 720

    # í…ìŠ¤íŠ¸
    title_font: str = "Pretendard-Bold"
    title_size: int = 72
    title_color: str = "#FFFFFF"
    title_stroke_color: str = "#000000"
    title_stroke_width: int = 3

    # ë°°ê²½
    overlay_color: str = "#000000"
    overlay_opacity: float = 0.4

    # ë ˆì´ì•„ì›ƒ
    text_position: str = "center"    # center, bottom
    padding: int = 40
    max_title_lines: int = 3
```

### 6.2 ì¸ë„¤ì¼ ìƒì„±ê¸°
```python
from PIL import Image, ImageDraw, ImageFont


class ThumbnailGenerator:
    def __init__(self, style: ThumbnailStyle | None = None):
        self.style = style or ThumbnailStyle()

    async def generate(
        self,
        title: str,
        background: VisualAsset | None,
        output_path: Path,
    ) -> Path:
        """ì¸ë„¤ì¼ ìƒì„±"""
        style = self.style

        # 1. ë°°ê²½ ì´ë¯¸ì§€ ì¤€ë¹„
        if background and background.path:
            bg = Image.open(background.path)
        elif background and background.url:
            import httpx
            async with httpx.AsyncClient() as client:
                response = await client.get(background.url)
                from io import BytesIO
                bg = Image.open(BytesIO(response.content))
        else:
            bg = Image.new("RGB", (style.width, style.height), "#1a1a2e")

        # ë¦¬ì‚¬ì´ì¦ˆ
        bg = bg.resize((style.width, style.height), Image.Resampling.LANCZOS)

        # 2. ì˜¤ë²„ë ˆì´ ì¶”ê°€
        overlay = Image.new("RGBA", bg.size, (*self._hex_to_rgb(style.overlay_color), int(style.overlay_opacity * 255)))
        bg = bg.convert("RGBA")
        bg = Image.alpha_composite(bg, overlay)

        # 3. í…ìŠ¤íŠ¸ ì¶”ê°€
        draw = ImageDraw.Draw(bg)

        try:
            font = ImageFont.truetype(style.title_font, style.title_size)
        except:
            font = ImageFont.load_default()

        # í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ
        wrapped_title = self._wrap_text(title, font, style.width - style.padding * 2)

        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚°
        bbox = draw.textbbox((0, 0), wrapped_title, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]

        if style.text_position == "center":
            x = (style.width - text_width) // 2
            y = (style.height - text_height) // 2
        else:  # bottom
            x = (style.width - text_width) // 2
            y = style.height - text_height - style.padding * 2

        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ì™¸ê³½ì„  + ë³¸ë¬¸)
        for dx in range(-style.title_stroke_width, style.title_stroke_width + 1):
            for dy in range(-style.title_stroke_width, style.title_stroke_width + 1):
                draw.text(
                    (x + dx, y + dy),
                    wrapped_title,
                    font=font,
                    fill=style.title_stroke_color
                )

        draw.text((x, y), wrapped_title, font=font, fill=style.title_color)

        # 4. ì €ì¥
        output_path = output_path.with_suffix(".jpg")
        bg.convert("RGB").save(output_path, "JPEG", quality=90)

        return output_path

    def _wrap_text(self, text: str, font, max_width: int) -> str:
        """í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ"""
        words = text.split()
        lines = []
        current_line = ""

        for word in words:
            test_line = f"{current_line} {word}".strip()
            bbox = font.getbbox(test_line)
            if bbox[2] <= max_width:
                current_line = test_line
            else:
                if current_line:
                    lines.append(current_line)
                current_line = word

        if current_line:
            lines.append(current_line)

        # ìµœëŒ€ ì¤„ ìˆ˜ ì œí•œ
        lines = lines[:self.style.max_title_lines]

        return "\n".join(lines)

    def _hex_to_rgb(self, hex_color: str) -> tuple:
        hex_color = hex_color.lstrip("#")
        return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))
```

---

## 7. ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©

```python
from pydantic import BaseModel
from pathlib import Path
from datetime import datetime


class VideoGenerationResult(BaseModel):
    """ì˜ìƒ ìƒì„± ê²°ê³¼"""
    video_path: Path
    thumbnail_path: Path
    duration_seconds: float

    # ë©”íƒ€
    script_id: str
    channel_id: str
    generated_at: datetime

    # ì‚¬ìš©ëœ ì—ì…‹ ì •ë³´
    tts_service: str
    visual_sources: list[str]


class VideoGenerationPipeline:
    """ì˜ìƒ ìƒì„± ì „ì²´ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        tts_factory: TTSEngineFactory,
        visual_manager: VisualSourcingManager,
        compositor: FFmpegCompositor,
        thumbnail_generator: ThumbnailGenerator,
    ):
        self.tts_factory = tts_factory
        self.visual_manager = visual_manager
        self.compositor = compositor
        self.thumbnail_generator = thumbnail_generator

    async def generate(
        self,
        script: "GeneratedScript",
        persona: "Persona",
        output_dir: Path,
    ) -> VideoGenerationResult:
        """ìŠ¤í¬ë¦½íŠ¸ â†’ ì™„ì„± ì˜ìƒ"""

        # ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        work_dir = output_dir / script.id
        work_dir.mkdir(parents=True, exist_ok=True)

        # 1. TTS ìƒì„±
        tts_engine = self.tts_factory.get_engine(persona.voice.service)
        tts_result = await tts_engine.synthesize(
            text=script.script,
            config=VoiceConfig(
                service=persona.voice.service,
                voice_id=persona.voice.voice_id,
                speed=persona.voice.voice_settings.get("speed", 1.0),
            ),
            output_path=work_dir / "audio",
        )

        # 2. ìë§‰ ìƒì„±
        subtitle_generator = SubtitleGenerator()
        if tts_result.word_timestamps:
            subtitle = subtitle_generator.generate_from_timestamps(
                tts_result.word_timestamps
            )
        else:
            subtitle = subtitle_generator.generate_from_script(
                script.script,
                tts_result.duration_seconds,
            )

        # 3. ë¹„ì£¼ì–¼ ì†Œì‹±
        visuals = await self.visual_manager.source_visuals(
            keywords=script.topic.keywords[:5],
            duration_needed=tts_result.duration_seconds,
            config=VisualConfig(),
        )

        # 4. ì˜ìƒ í•©ì„±
        video_path = await self.compositor.compose(
            audio=tts_result,
            subtitle=subtitle,
            visuals=visuals,
            output_path=work_dir / f"{script.id}.mp4",
        )

        # 5. ì¸ë„¤ì¼ ìƒì„±
        thumbnail_path = await self.thumbnail_generator.generate(
            title=script.topic.title,
            background=visuals[0] if visuals else None,
            output_path=work_dir / "thumbnail",
        )

        return VideoGenerationResult(
            video_path=video_path,
            thumbnail_path=thumbnail_path,
            duration_seconds=tts_result.duration_seconds,
            script_id=script.id,
            channel_id=script.channel_id,
            generated_at=datetime.utcnow(),
            tts_service=persona.voice.service,
            visual_sources=[v.source for v in visuals if v.source],
        )
```

---

## 8. Scene ê¸°ë°˜ ì˜ìƒ ìƒì„± ì‹œìŠ¤í…œ (BSForge ì°¨ë³„í™”)

### 8.1 ê°œìš”
BSForgeì˜ í•µì‹¬ ì°¨ë³„í™” ìš”ì†ŒëŠ” **AI í˜ë¥´ì†Œë‚˜ê°€ ì‚¬ì‹¤(Fact)ê³¼ ì˜ê²¬(Opinion)ì„ êµ¬ë¶„í•˜ì—¬ í‘œí˜„**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. Scene ê¸°ë°˜ ì‹œìŠ¤í…œì€ ì´ë¥¼ ì‹œê°ì ìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.

### 8.2 SceneType (ì¥ë©´ ìœ í˜•)
```python
class SceneType(str, Enum):
    HOOK = "hook"              # ì˜¤í”„ë‹ í›… (ì‹œì„  ëŒê¸°)
    INTRO = "intro"            # ì£¼ì œ ì†Œê°œ
    CONTENT = "content"        # ë³¸ë¡  (ì‚¬ì‹¤ ì „ë‹¬)
    EXAMPLE = "example"        # ì˜ˆì‹œ/ì‚¬ë¡€
    COMMENTARY = "commentary"  # AI í˜ë¥´ì†Œë‚˜ ì˜ê²¬/í•´ì„
    REACTION = "reaction"      # ê°ì •ì  ë°˜ì‘
    CONCLUSION = "conclusion"  # ê²°ë¡ /ìš”ì•½
    CTA = "cta"               # Call to Action
```

### 8.3 VisualStyle (ì‹œê°ì  ìŠ¤íƒ€ì¼)
```python
class VisualStyle(str, Enum):
    NEUTRAL = "neutral"    # ì‚¬ì‹¤ ì „ë‹¬ (ê¸°ë³¸ ìŠ¤íƒ€ì¼)
    PERSONA = "persona"    # AI ì˜ê²¬ (ê°•ì¡° ìŠ¤íƒ€ì¼: í…Œë‘ë¦¬, ì•…ì„¼íŠ¸ ì»¬ëŸ¬)
    EMPHASIS = "emphasis"  # í•µì‹¬ ê²°ë¡  (í™”ë©´ ê°€ë“ í…ìŠ¤íŠ¸)
```

### 8.4 TransitionType (ì „í™˜ íš¨ê³¼)
```python
class TransitionType(str, Enum):
    NONE = "none"
    FADE = "fade"          # í˜ì´ë“œ ì¸/ì•„ì›ƒ
    CROSSFADE = "crossfade"
    ZOOM = "zoom"
    FLASH = "flash"        # ì‚¬ì‹¤â†’ì˜ê²¬ ì „í™˜ ì‹œ í”Œë˜ì‹œ íš¨ê³¼
    SLIDE = "slide"
```

### 8.5 Scene ê¸°ë°˜ íŒŒì´í”„ë¼ì¸
```
SceneScript ìƒì„± (LLM)
    â†“
Sceneë³„ TTS ìƒì„±
    â†“
Sceneë³„ ìë§‰ ìƒì„± (ìŠ¤íƒ€ì¼ ë¶„ê¸°)
    â†“
Sceneë³„ ë¹„ì£¼ì–¼ ì†Œì‹±
    â†“
Sceneë³„ íŠ¸ëœì§€ì…˜ ì ìš©
    â†“
FFmpeg í•©ì„±
```

### 8.6 ì‹œê°ì  ì°¨ë³„í™” ì˜ˆì‹œ
| Scene Type | Visual Style | íŠ¹ì§• |
|------------|--------------|------|
| CONTENT | NEUTRAL | ê¸°ë³¸ ë°°ê²½, í°ìƒ‰ ìë§‰ |
| COMMENTARY/REACTION | PERSONA | ì•…ì„¼íŠ¸ ì»¬ëŸ¬ í…Œë‘ë¦¬, ê°•ì¡° íš¨ê³¼ |
| CONCLUSION | EMPHASIS | í™”ë©´ ì¤‘ì•™ í° í…ìŠ¤íŠ¸ |

### 8.7 ìë™ íŠ¸ëœì§€ì…˜ ì¶”ì²œ
```python
def apply_recommended_transitions(scenes: list[Scene]) -> list[Scene]:
    """ì—°ì†ëœ Scene ê°„ ì ì ˆí•œ íŠ¸ëœì§€ì…˜ ìë™ ì ìš©"""
    for i, scene in enumerate(scenes[1:], 1):
        prev = scenes[i-1]

        # ì‚¬ì‹¤ â†’ ì˜ê²¬: í”Œë˜ì‹œ íš¨ê³¼
        if prev.visual_style == VisualStyle.NEUTRAL and scene.visual_style == VisualStyle.PERSONA:
            scene.transition_in = TransitionType.FLASH

        # ì˜ê²¬ â†’ ì‚¬ì‹¤: í˜ì´ë“œ
        elif prev.visual_style == VisualStyle.PERSONA and scene.visual_style == VisualStyle.NEUTRAL:
            scene.transition_in = TransitionType.FADE

        # HOOK â†’ ë³¸ë¡ : ì¤Œ
        elif prev.scene_type == SceneType.HOOK:
            scene.transition_in = TransitionType.ZOOM
```

---

## 9. BGM (ë°°ê²½ ìŒì•…) ì‹œìŠ¤í…œ

### 9.1 ê°œìš”
ìë™ìœ¼ë¡œ YouTubeì—ì„œ ë¡œì—´í‹° í”„ë¦¬ BGMì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì˜ìƒì— ë¯¹ì‹±í•©ë‹ˆë‹¤.

### 9.2 BGM ì„¤ì •
```python
class BGMTrack(BaseModel):
    """ê°œë³„ BGM íŠ¸ë™ ì •ë³´."""
    name: str                        # íŠ¸ë™ ì‹ë³„ ì´ë¦„
    youtube_url: HttpUrl             # YouTube URL
    tags: list[str] = []             # ë¶„ìœ„ê¸° íƒœê·¸ (upbeat, calm, tech ë“±)
    default_volume: float = 0.15     # ê¸°ë³¸ ë³¼ë¥¨ (0.0-1.0)

class BGMConfig(BaseModel):
    """BGM ì‹œìŠ¤í…œ ì„¤ì •."""
    enabled: bool = True
    volume: float = 0.15             # ì „ì²´ BGM ë³¼ë¥¨
    cache_dir: str = "data/bgm"      # ë‹¤ìš´ë¡œë“œ ìºì‹œ ê²½ë¡œ
    download_timeout: int = 300      # ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    tracks: list[BGMTrack] = []      # ì‚¬ìš©í•  íŠ¸ë™ ëª©ë¡
```

### 9.3 BGM ë‹¤ìš´ë¡œë”
```python
class BGMDownloader:
    """YouTubeì—ì„œ BGM ë‹¤ìš´ë¡œë“œ (yt-dlp ì‚¬ìš©)."""

    async def download(self, track: BGMTrack) -> Path:
        """íŠ¸ë™ ë‹¤ìš´ë¡œë“œ (ìºì‹œëœ ê²½ìš° ìŠ¤í‚µ)."""
        output_path = self.config.get_cache_path(track)

        if output_path.exists():
            return output_path

        # yt-dlpë¡œ ì˜¤ë””ì˜¤ë§Œ ì¶”ì¶œ â†’ MP3 ë³€í™˜
        ydl_opts = {
            "format": "bestaudio/best",
            "postprocessors": [{
                "key": "FFmpegExtractAudio",
                "preferredcodec": "mp3",
                "preferredquality": "192",
            }],
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([track.youtube_url])

        return output_path

    async def ensure_all_downloaded(self, tracks: list[BGMTrack]) -> dict[str, Path]:
        """ëª¨ë“  íŠ¸ë™ ë‹¤ìš´ë¡œë“œ ë³´ì¥."""
        results = {}
        for track in tracks:
            results[track.name] = await self.download(track)
        return results
```

### 9.4 BGM ì„ íƒê¸°
```python
class BGMSelector:
    """ì˜ìƒì— ë§ëŠ” BGM ì„ íƒ."""

    def select(self, tags: list[str] | None = None) -> tuple[BGMTrack, Path] | None:
        """íƒœê·¸ ê¸°ë°˜ BGM ì„ íƒ (ë¯¸ë˜: ë¶„ìœ„ê¸° ë§¤ì¹­)."""
        if not self._cached_tracks:
            return None

        # í˜„ì¬: ëœë¤ ì„ íƒ
        # TODO: íƒœê·¸ ë§¤ì¹­, ì‹œë¦¬ì¦ˆ ì¼ê´€ì„± ë“±
        track = random.choice(list(self._cached_tracks.values()))
        return track
```

### 9.5 BGM ë§¤ë‹ˆì €
```python
class BGMManager:
    """BGM íŒŒì´í”„ë¼ì¸ í†µí•© ê´€ë¦¬."""

    async def initialize(self) -> None:
        """ì‹œì‘ ì‹œ ëª¨ë“  íŠ¸ë™ ë‹¤ìš´ë¡œë“œ."""
        self._cached_tracks = await self._downloader.ensure_all_downloaded(
            self.config.tracks
        )
        self._selector = BGMSelector(self.config, self._cached_tracks)

    async def get_bgm_for_video(self, mood_tags: list[str] | None = None) -> Path | None:
        """ì˜ìƒìš© BGM ê²½ë¡œ ë°˜í™˜."""
        if not self.config.enabled:
            return None

        result = self._selector.select(tags=mood_tags)
        return result[1] if result else None

    def get_volume(self) -> float:
        """ì„¤ì •ëœ ë³¼ë¥¨ ë°˜í™˜."""
        return self.config.volume
```

### 9.6 FFmpeg ë¯¹ì‹±
```python
# ìŒì„± + BGM ë¯¹ì‹± ëª…ë ¹ì–´
ffmpeg_cmd = [
    "ffmpeg", "-y",
    "-i", str(video_path),           # ì›ë³¸ ì˜ìƒ (ìŒì„± í¬í•¨)
    "-i", str(bgm_path),             # BGM
    "-filter_complex",
    f"[1:a]volume={bgm_volume}[bgm];"   # BGM ë³¼ë¥¨ ì¡°ì ˆ
    "[0:a][bgm]amix=inputs=2:duration=first[aout]",  # ë¯¹ì‹±
    "-map", "0:v",                   # ì›ë³¸ ë¹„ë””ì˜¤
    "-map", "[aout]",                # ë¯¹ì‹±ëœ ì˜¤ë””ì˜¤
    "-c:v", "copy",                  # ë¹„ë””ì˜¤ ì¬ì¸ì½”ë”© ì•ˆí•¨
    "-c:a", "aac",
    str(output_path),
]
```

---

## 10. êµ¬í˜„ ìƒì„¸

### 10.1 ì‹¤ì œ êµ¬í˜„ëœ ëª¨ë“ˆ

**TTS Services (`app/services/generator/tts/`)**:
- `BaseTTSEngine`: ì¶”ìƒ ê¸°ë°˜ í´ë˜ìŠ¤
- `EdgeTTSEngine`: ë¬´ë£Œ Microsoft Edge TTS
- `ElevenLabsEngine`: ê³ í’ˆì§ˆ ìœ ë£Œ TTS
- `TTSEngineFactory`: ì„œë¹„ìŠ¤ ì„ íƒ íŒ©í† ë¦¬

**Subtitle (`app/services/generator/subtitle.py`)**:
- `SubtitleGenerator`: ASS/SRT ìƒì„±
- `SubtitleStyle`: ìŠ¤íƒ€ì¼ ì„¤ì • (í°íŠ¸, ìƒ‰ìƒ, ìœ„ì¹˜)
- ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 

**Visual (`app/services/generator/visual/`)**:
- `PexelsClient`: ìŠ¤í†¡ ì˜ìƒ/ì´ë¯¸ì§€ ê²€ìƒ‰
- `AIImageGenerator`: DALL-E ì´ë¯¸ì§€ ìƒì„±
- `VisualSourcingManager`: ì†Œì‹± ìš°ì„ ìˆœìœ„ ê´€ë¦¬

**Compositor (`app/services/generator/`)**:
- `FFmpegWrapper`: FFmpeg ëª…ë ¹ì–´ ë˜í¼
- `VideoCompositor`: ì „ì²´ í•©ì„± íŒŒì´í”„ë¼ì¸
- Sceneë³„ íŠ¸ëœì§€ì…˜ ì ìš©

**BGM (`app/services/generator/bgm/`)**:
- `BGMDownloader`: yt-dlp ê¸°ë°˜ ë‹¤ìš´ë¡œë”
- `BGMSelector`: íŠ¸ë™ ì„ íƒ ë¡œì§
- `BGMManager`: í†µí•© ê´€ë¦¬ì

**Scene (`app/models/scene.py`)**:
- `SceneType`: 8ê°€ì§€ ì¥ë©´ ìœ í˜•
- `VisualStyle`: 3ê°€ì§€ ì‹œê° ìŠ¤íƒ€ì¼
- `TransitionType`: 6ê°€ì§€ ì „í™˜ íš¨ê³¼
- `Scene`, `SceneScript`: Pydantic ëª¨ë¸

### 10.2 íŒŒì¼ êµ¬ì¡°
```
app/services/generator/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ pipeline.py              # ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©
â”œâ”€â”€ compositor.py            # FFmpeg í•©ì„±
â”œâ”€â”€ ffmpeg.py               # FFmpeg ë˜í¼
â”œâ”€â”€ subtitle.py             # ìë§‰ ìƒì„±
â”œâ”€â”€ thumbnail.py            # ì¸ë„¤ì¼ ìƒì„±
â”œâ”€â”€ tts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py             # BaseTTSEngine
â”‚   â”œâ”€â”€ edge.py             # EdgeTTSEngine
â”‚   â”œâ”€â”€ elevenlabs.py       # ElevenLabsEngine
â”‚   â””â”€â”€ factory.py          # TTSEngineFactory
â”œâ”€â”€ visual/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pexels.py           # PexelsClient
â”‚   â”œâ”€â”€ ai_image.py         # AIImageGenerator
â”‚   â””â”€â”€ manager.py          # VisualSourcingManager
â””â”€â”€ bgm/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ downloader.py       # BGMDownloader
    â”œâ”€â”€ selector.py         # BGMSelector
    â””â”€â”€ manager.py          # BGMManager
```

---

## 11. ê¸°ìˆ  ìŠ¤íƒ ì •ë¦¬

| ì»´í¬ë„ŒíŠ¸ | ë¼ì´ë¸ŒëŸ¬ë¦¬ | ë¹„ê³  |
|----------|------------|------|
| **TTS** | edge-tts, elevenlabs | ë¬´ë£Œ/ìœ ë£Œ |
| **ìë§‰** | ìì²´ ASS/SRT ìƒì„± | ìŠ¤íƒ€ì¼ë§ ì§€ì› |
| **ë¹„ì£¼ì–¼** | httpx (Pexels API), openai (DALL-E) | ìŠ¤í†¡/AI |
| **ì´ë¯¸ì§€ ì²˜ë¦¬** | Pillow | ì¸ë„¤ì¼, ë‹¨ìƒ‰ ë°°ê²½ |
| **ì˜ìƒ í•©ì„±** | FFmpeg (subprocess) | í•µì‹¬ |
| **ìŒì„± ì¸ì‹** | whisper | íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± |
| **BGM ë‹¤ìš´ë¡œë“œ** | yt-dlp | YouTube ì˜¤ë””ì˜¤ ì¶”ì¶œ |
| **Scene ëª¨ë¸** | Pydantic | íƒ€ì… ì•ˆì „ì„± |
